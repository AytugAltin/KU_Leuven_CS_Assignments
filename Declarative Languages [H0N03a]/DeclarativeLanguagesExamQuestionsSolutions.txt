Alle theoretische examenvragen van de afgelopen jaren.
Antwoorden moeten nog wat verfijnd worden :-)

Bespreek het verschil tussen unificatie en pattern matching.
    Het basisidee is dat pattern matching in 1 richting werkt en unificatie in beide richtingen. Je kan dus bijvoorbeeld een ongebonden variabele unificeren met een andere variabele als volgt: X = Y, X = 5. Y was nog niet gebonden maar heeft uiteindelijk toch de waarde 5. In Haskell, dat met pattern matching werkt, zou er een fout gegeven worden by X = Y. 

    In Haskell wordt pattern matching gebruikt in de definitie van functies, bv. in qsort: matching qsort [x:xs] met [3,2,4,1,5] resulteert in de binding van  x aan 3 en van xs aan [2,4,1,5]. Ook by typechecking komt een dergelijke situatie voor: als men een functie f wil toepassen op een argument arg, dan moet de typedescriptor van f van de vorm td1 td2 zijn, en als arg typedescriptor tdarg heeft, dan moeten td1 en tdarg "gelijk gemaakt" kunnen worden, door vereenvoudigingen, maar ook door de vervanging van typevariabelen door meer concrete typedescriptoren. Dit "gelijk maken" (en tegelijk variabelen binden) noemt men unificatie

Wat doet backtracking in Prolog.
    Backtracking is een mechanisme dat toelaat alle mogelijke oplossingen voor een query te vinden. Er wordt bij het uitvoeren van een query namelijk een uitvoeringsboom opgesteld waar elk element van de boom een deelquery is. Als zo'n deelquery slaagt, wordt er aan een ongebonden variabele een waarde gegeven en wordt de uitvoeringsboom uitgediept. Als een deelquery faalt wordt er aan de laatst toegekende ongebonden variabele een andere waarde gegeven. Als alle mogelijke waarden geprobeerd zijn faalt de deelquery en wordt er terug naar boven 'gebacktracked'. 

Wat zijn de voor en nadelen van luie uitvoering in Haskell?
    + Mogelijkheid tot oneindige datastructuren (bvb take 5 [1..] of fibs)
    + Hogere performantie: onnodige berekeningen worden vermeden (bvb head (map (2 *) [1 .. 10]) enkel het eerste element van de lijst wordt berekend)
    - Debuggen wordt moeilijker
    - Het wordt moeilijker om de snelheid en het geheugengebruik tijdens de runtime van je programma te voorspellen.

    It means that expressions are not evaluated when they are bound to variables, but their evaluation is deferred until their results are needed by other computations. In consequence, arguments are not evaluated before they are passed to a function, but only when their values are actually used
    
    -its main drawback is that memory usage becomes hard to predict. 
    +Mostly because it can be more efficient -- values don't need to be computed if they're not going to be used.
    +allows one to bypass undefined values (e.g. results of infinite loops) and in this way it also allows one to process formally infinite data.
    - introduces an overhead that leads programmers to hunt for places where they can make their code more strict
    + making the right things efficient enough
    + commonplace to define infinite structures, and then only use as much as is needed, rather than having to mix up the logic of constructing the data structure with code that determines whether any part is needed. Code modularity is increased, as laziness gives you more ways to chop up your code into small pieces, each of which does a simple task of generating, filtering, or otherwise manipulating data.

    Can make qualitative improvements to performance!
    Can hurt performance in some other cases.
    Makes code simpler.
    Makes hard problems conceivable.
    Allows for separation of concerns with regard to generating and processing data.

Hoe is in Haskell sprake van generisch programmeren
    Haskell heeft typeklassen zoals Eq (equivalentie) en Show (vergelijkbaar met een toString() in Java). Als je bijvoorbeeld een nieuw type BTree aanmaakt als volgt:
        data BTree a = Leaf a | Node (BTree a) a (BTree a) deriving (Eq, Show).
    BTree heeft dan sowieso een definitie van == en show voor elke BTree van type T als T zelf deze operaties ondersteunt. Een eenvoudiger voorbeeld van generisch  programmeren is bijvoorbeeld de functie
        lengte :: [a] -> Integer.
    a is hier een generisch type parameter. De functie lengte is als volgt verder gedefinieerd:
        lengte [] = 0
        lengte (h:r) = 1 + lengte r
    Deze functie kan dus gebruikt worden voor elk type a en is dus generisch.

    Haskell is a polymorphic language. This means that you can have a single datatype for lists:
        data List a = Nil | Cons a (List a)
    These lists can contain any type of information, such as integers, Booleans, or even other lists. Since the length of a list does not depend on the type of its elements, there is also a single definition for list length:
        length :: List a -> Int
        length Nil        = 0
        length (Cons _ t) = 1 + length t
    The length function can be defined on other datatypes than lists. Consider a datatype for trees:
        data Tree a = Leaf | Bin a (Tree a) (Tree a)
    You can also compute the length of a tree (or its size, if you want), by recursively traversing the tree and counting the number of elements. Generic programming allows to define a single length function, that can operate on lists, trees, and many other datatypes. This reduces code duplication and makes code more robust to changes, because you can change your datatypes without needing to adapt the generic functions that operate on them. 

    Generic programming support in GHC allows defining classes with methods that do not need a user specification when instantiating: the method body is automatically derived by GHC. This is similar to what happens for standard classes such as Read and Show, for instance, but now for user-defined classes. 

Wanneer doet prolog aan garbage collecting
    Prolog doet aan garbage collection als er gebacktracked wordt in de uitvoeringsboom. Takken die volledig uitgediept zijn (en dus niet meer nodig) worden verwijderd uit de uitvoeringsboom en de bijhorende variabelen worden dus verwijderd uit het geheugen. Dit gebeurt bijgevolg automatisch, er is geen ingewikkeld algoritme nodig om te bepalen welke geheugenobjecten gedealloceerd mogen worden (in tegenstelling tot Java en in mindere mate ook Haskell).    
        Triples that have died before the the generation of last still active query.
        Entailment matrices for rdfs:subPropertyOf relations that are related to old queries.

Bespreek meta-programmatie in Prolog.
    De twee basispredicaten voor meta-programmatie zijn =.. en call. Een simpel voorbeeld is een implementatie van map:
         map(FunctionName,[H|T],[NH|NT]):-
             Function=..[FunctionName,H,NH],
             call(Function), map(FunctionName,T,NT).
         map(_,[],[]).
    = .. laat dus toe een een predicaat samen met zijn argumenten op te slaan in een lijst. call roept dan het predicaat (1e element van de lijst) op met de meegegeven argumenten (andere elementen van de lijst). Een ander handig predicaat is clause(H,B) dat in de huidige kennisbank van Prolog zoekt naar een clause met een hoofd dat unificeert met H en een body dat unificeert met B. Als de clause geen body heeft is B true. 

Bespreek de functie van typeklassen in Mercury/Haskell.
    Typeklassen zoals Eq, Show en Read laten toe om voor verschillende types een bepaalde gemeenschappelijke functie te definieren. Bijvoorbeeld de gelijkheid == is voor bijna elk type handig maar is ook voor bijna elk type verschillend. In plaats van verschillende equivalentie functies zoals
        equalsInteger:: Integer -> Integer -> Bool
    en
        equalsString :: String -> String -> Bool
    te definieren kan je met behulp van typeklassen een instantie van Eq aanmaken voor Integers en String. Een vb:
        instance Eq Integer where  
        x == y   =  x `integerEq` y

Waarom gebruikt prolog clause indexing?
Indexing is a technique used to quickly select candidate clauses of a predicate for a specific goal. In most Prolog systems, indexing is done (only) on the first argument of the head. If this argument is instantiated to an atom, integer, float or compound term with functor, hashing is used to quickly select all clauses where the first argument may unify with the first argument of the goal. SWI-Prolog supports just-in-time and multi-argument indexing. 
SWI-Prolog provides `just-in-time' indexing over multiple arguments. `Just-in-time' means that clause indexes are not built by the compiler, but on the first call to such a predicate where an index might help (i.e., a call where at least one argument is instantiated).
List clause selection process for various predicates and calls: 
    -Special purpose code 
    -Linear scan on first argument: The principal clause list maintains a key for the first argument. An indexing key is either a constant or a functor (name/arity reference).  Calls with an instantiated first argument and less than 10 clauses perform a linear scan for a possible matching clause using this index key. 
    -Hash lookup: Indexing technique used for quick lookup.
        the system considers the available hash tables for which the corresponding argument is instantiated. If a table is found with acceptable characteristics, it is used. Otherwise, there are two cases. First, if no hash table is available for the instantiated arguments, it assesses the clauses for all instantiated arguments and selects the best candidate for creating a hash table. Arguments that cannot be indexed are flagged to avoid repeated scanning. Second, if there is a hash table for an indexed argument but it has poor characteristics, the system scans other instantiated arguments to see whether it can create a better hash table. The system maintains a bit vector on each table in which it marks arguments that are less suitable than the argument to which the table belongs. 

Bespreek het verschil tussen pattern matching en guards.
    Actually, they're fundamentally quite different! At least in Haskell, at any rate.

    Guards are both simpler and more flexible: They're essentially just special syntax that translates to a series of if/then expressions. You can put arbitrary boolean expressions in the guards, but they don't do anything you couldn't do with a regular if.

    Pattern matches do several additional things: They're the only way to deconstruct data, and they bind identifiers within their scope. In the same sense that guards are equivalent to if expressions, pattern matching is equivalent to case expressions. Declarations (either at the top level, or in something like a let expression) are also a form of pattern match, with "normal" definitions being matches with the trivial pattern, a single identifier.

    Pattern matches also tend to be the main way stuff actually happens in Haskell--attempting to deconstruct data in a pattern is one of the few things that forces evaluation.

Waarom werken we met failure-driven loop?
    The idea of failure driven loops is to force Prolog to backtrack until there are no more possibilities left. Failure driven loops, for example to read data. 
    These loops are useful only when used in conjunction with extra-logical predicates dthat cause side-effects. Example: is query Goal, write (Goal),nl,fail?, which causes all solutions to a goal to be written on the screen. Or between(1,5,X) Prolog will backtrack and try again with a different solution. Hey, that’s a loop!.

Welke invloed heeft prescriptieve modes op de efficiëntie in Mercury?
    There are two ways to view modes, the descriptive view and the prescriptive view. In the descriptive view, modes describe the instantiation states of the arguments of a predicate when execution enters the predicate and possibly when execution leaves the predicate. In the prescriptive view,, modes assign responsibility for the instantiation of variables to the various subgoals of a predicate defnition. Unlike descriptive modes, prescriptive modes can afect the computation rule. prescriptieve modes= controls the execution order and support coroutines. 
    Prescriptive view: modes allocate responsibility to instantiate (each part of) each argument, to either this predicate or its caller.
    Descriptive view: modes describe the instantiation states of arguments at entry to and (possibly) return from this predicate.
    Mercury takes the prescriptive view, which is order independent, and in which modes convey design level information.
    In a prescriptive mode system, mode analysis is also an important part of the semantic analysis phase of compilation (much like type analysis) and can inform the programmer of many errors or potential errors in the program at compile time. We therefore believe it is an essential component of any industrial strength logic programming system. 

Waarom gebruiken we in Prolog de 'most general unifier' (mgu) en niet een unifier?
    That do just enough substitution to unify and no more.

Bespreek het verschil tussen open-ended lists en difference lists
    One is a concrete term, the other rather a convention.

    Open-ended lists are terms that are not lists but can be instantiated such that they become lists. In standard lingo, they are called partial lists. Here are partial lists: X, [a|X], [X|X] , [a,b,c | _] are all partial lists.
    The notion open-ended lists suggests a certain usage of such lists to simulate some open-ended state. Think of a dictionary that might be represented by an open-ended list. Every time you add a new item, the variable "at the end of the partial list" is instantiated to a new element. While this programming technique is quite possible in Prolog, it has one big downside: The programs will heavily depend on a procedural interpretation. And in many situations there is no way to have a declarative interpretation at all.
    
    Difference lists are effectively not lists as such but a certain way how lists are used such that the intended list is represented by two variables: one for the start and one for the end of the list. For this reason it would help a lot to rather talk of list differences instead of difference listsù
    Consider: el(E, [E|L],L).
    Here, the last two arguments can be seen as forming a difference: a list that contains the single element [E]. You can now construct more complex lists out of simpler ones, provided you respect certain conventions which are essentially that the second argument is only passed further on. 

    Open list is a tool used to implement a difference list. Open list is any list where you have a unassigned variable at some point in the list, e.g.: [a,b,c|X]. You can use open list to implement a data structure called difference list, which formally specifies a structure of two terms pointing to first element and to the open end, traditionally defined as: [a,b,c|X]-X, to make operating on such lists easier.
    For example, if all you have is an open list, adding element to the end is possible, but you need to iterate over all items. In a difference list you can just use the end-of-list variable (called a Hole on the page above) to skip iteration and perform the operation in constant time.

Bespreek het verschil tussen patterns en constructors
    Nog verder aanvullen...
    Constructers are used:
        -To create values (rhs of an equation; to group values together)
        -In patterns to separate the values (in lhs of the equation)

Geef de definitie van een declaratieve programeertaal
    Het verschil dat men in een declaratieve taal opschrijft wat er aan de hand is, en in een imperatieve taal hoe iets moet gebeuren. Een taal is declaratief als de basiselementen geen opdrachten zijn, maar beschrijvingen die tot ingewikkelder beschrijvingen worden samengesteld. Daarmee is nog niets gezegd over de aard van die beschrijvingen. Er is daarom een groot spectrum aan declaratieve talen.

Geef het meest algemene type van >>= en leg informeel met eigen woorden uit?
    >>= noemt infix operator, Infix operators are really just functions, and can also be defined using equations
    (>>=) :: (Monad m) => m a -> (a -> m b) -> m b  
    We write m a instead of f a because the m stands for Monad, but monads are just applicative functors that support >>=. The >>= function is pronounced as bind. 
    >>= takes a monadic value, and a function that takes a normal value and returns a monadic value and manages to apply that function to the monadic value
    
    The Monad bind operator is a function type signature. A type signature is like an abstract method defined in an abstract class:
    1)A Monad that contains type a as the first input
    2)A function (a -> m b) as the second input. (a -> m b) is a first order function that takes a as input, and returns Monad m b. You can think of this as a delegate design pattern in Object-Oriented world, except you don’t need to pass in a whole object, but only a function itself is sufficient.
    3)The implementation will return Monad m b as a result.

    The next function is >>=, or bind. It's like function application, only instead of taking a normal value and feeding it to a normal function, it takes a monadic value (that is, a value with a context) and feeds it to a function that takes a normal value but returns a monadic value. 

    (The signature of >>= helps us to understand this operation: ma >>= \v -> mb combines a monadic value ma containing values of type a and a function which operates on a value v of type a, returning the monadic value mb. The result is to combine ma and mb into a monadic value containing b)

    Its first argument is a value in a monadic type, its second argument is a function that maps from the underlying type of the first argument to another monadic type, and its result is in that other monadic type. Typically, the binding operation can be understood as having four stages:

    1)The monad-related structure on the first argument is "pierced" to expose any number of values in the underlying type t.
    2)The given function is applied to all of those values to obtain values of type (M u).
    3)The monad-related structure on those values is also pierced, exposing values of type u.
    4)Finally, the monad-related structure is reassembled over all of the results, giving a single value of type (M u).

   Extra:
    Ook al heeft call-by-need meerdere interessante eigenschappen, is het toch niet de meest geprefereerde uitvoeringsstrategie. Er zijn namelijk ook een aantal nadelen aan verbonden:
    Er is meer overhead verbonden aan de implementatie van call-by-need (het beheer van de thunks), dan bij call-by-value. Voor programma's waar callby-value en call-by-name ongeveer evenveel uitvoeringsstappen tellen, zijn
    call-by-value implementaties typisch sneller. De overhead wordt deels teniet gedaan door geoptimaliseerde compilatie:
    striktheidsanalyse gaat na wanneer call-by-value uitvoering kan worden. Voor programmeurs is het minder makkelijk om te redeneren over de uitvoeringskarakteristieken
    (tijd en ruimte) van call-by-need, dan van callby-value. De combinatie van call-by-need met I/O en andere neveneecten is complexer.